# ultra_simple_server.py
from flask import Flask, request, jsonify
from flask_cors import CORS
import os
import sys

app = Flask(__name__)
CORS(app)

print("üîß –£–õ–¨–¢–†–ê-–ü–†–û–°–¢–û–ô –°–ï–†–í–ï–† –î–õ–Ø T5")

# –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä–∏–º –±–∞–∑–æ–≤—ã–µ –∏–º–ø–æ—Ä—Ç—ã
try:
    from transformers import T5Tokenizer, T5ForConditionalGeneration
    import torch
    print("‚úÖ –ë–∞–∑–æ–≤—ã–µ –∏–º–ø–æ—Ä—Ç—ã —É—Å–ø–µ—à–Ω—ã")
except ImportError as e:
    print(f"‚ùå –û—à–∏–±–∫–∞ –∏–º–ø–æ—Ä—Ç–∞: {e}")
    sys.exit(1)

# –ü—É—Ç—å –∫ –º–æ–¥–µ–ª–∏
model_path = r"C:\Users\–ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å\Downloads\PromptCraft-Judge-T5-Base-V1-20251116T171005Z-1-001\PromptCraft-Judge-T5-Base-V1\checkpoint-1000"

print(f"üìÅ –ü—É—Ç—å –∫ –º–æ–¥–µ–ª–∏: {model_path}")

# –ü—Ä–æ–≤–µ—Ä–∏–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤
required_files = ['config.json', 'model.safetensors', 'tokenizer_config.json']
for file in required_files:
    file_path = os.path.join(model_path, file)
    if os.path.exists(file_path):
        print(f"‚úÖ {file} - –Ω–∞–π–¥–µ–Ω")
    else:
        print(f"‚ùå {file} - –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç")

print("\nüîÑ –ü—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä...")

try:
    # –ü—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä —Ä–∞–∑–Ω—ã–º–∏ —Å–ø–æ—Å–æ–±–∞–º–∏
    print("–°–ø–æ—Å–æ–± 1: –ü—Ä—è–º–∞—è –∑–∞–≥—Ä—É–∑–∫–∞...")
    tokenizer = T5Tokenizer.from_pretrained(model_path)
    print("‚úÖ –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω!")
    
except Exception as e:
    print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
    print("–ü—Ä–æ–±—É–µ–º –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π —Å–ø–æ—Å–æ–±...")
    
    try:
        # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π —Å–ø–æ—Å–æ–±
        from transformers import AutoTokenizer
        tokenizer = AutoTokenizer.from_pretrained(model_path)
        print("‚úÖ –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω —á–µ—Ä–µ–∑ AutoTokenizer!")
    except Exception as e2:
        print(f"‚ùå –ò —ç—Ç–æ—Ç —Å–ø–æ—Å–æ–± –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª: {e2}")
        print("–ò—Å–ø–æ–ª—å–∑—É–µ–º –±–∞–∑–æ–≤—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä T5...")
        tokenizer = T5Tokenizer.from_pretrained("t5-base")
        print("‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ–º –±–∞–∑–æ–≤—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä T5")

print("\nüîÑ –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å...")

try:
    model = T5ForConditionalGeneration.from_pretrained(model_path)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)
    model.eval()
    print(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –Ω–∞ {device}!")
except Exception as e:
    print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
    sys.exit(1)

@app.route('/api/improve-prompt', methods=['POST'])
def improve_prompt():
    try:
        data = request.get_json()
        prompt = data.get('prompt', '').strip()
        
        if not prompt:
            return jsonify({"error": "Prompt is empty"}), 400
        
        print(f"üì® –ü–æ–ª—É—á–µ–Ω –ø—Ä–æ–º–ø—Ç: {prompt}")
        
        # –ü—Ä–æ—Å—Ç–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è
        input_text = f"improve_prompt: {prompt}"
        input_ids = tokenizer.encode(input_text, return_tensors="pt")
        input_ids = input_ids.to(device)
        
        with torch.no_grad():
            output_ids = model.generate(
                input_ids,
                max_length=128,
                num_beams=3,
                early_stopping=True
            )
        
        advice = tokenizer.decode(output_ids[0], skip_special_tokens=True)
        
        return jsonify({
            "original": prompt,
            "improved": advice,
            "status": "success"
        })
        
    except Exception as e:
        return jsonify({"error": str(e)}), 500

@app.route('/api/health', methods=['GET'])
def health_check():
    return jsonify({"status": "ok", "model": "loaded"})

@app.route('/')
def home():
    return "ü§ñ PromptCraft API - –†–∞–±–æ—Ç–∞–µ—Ç!"

if __name__ == '__main__':
    print("\nüéâ –°–ï–†–í–ï–† –ó–ê–ü–£–©–ï–ù: http://localhost:5000")
    app.run(port=5000, host='0.0.0.0')